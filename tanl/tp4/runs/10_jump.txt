C:\Users\qjoao\anaconda3\python.exe C:\Users\qjoao\Documents\GitHub\master_cours_choix\tanl\tp4\finetune_tp4.py 
Portuguese
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'B-LOC': 0, 'I-PER': 1, 'I-ORG': 2, 'O': 3, 'I-LOC': 4, 'B-ORG': 5, 'B-PER': 6}
Training model
Model saved

Chinese
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'B-LOC': 0, 'I-PER': 1, 'I-ORG': 2, 'O': 3, 'I-LOC': 4, 'B-ORG': 5, 'B-PER': 6}
Training model
Model saved

Swedish
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'B-LOC': 0, 'I-PER': 1, 'I-ORG': 2, 'O': 3, 'I-LOC': 4, 'B-ORG': 5, 'B-PER': 6}
Training model
Model saved

Serbian
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'B-LOC': 0, 'I-PER': 1, 'I-ORG': 2, 'O': 3, 'B-OTH': 4, 'I-OTH': 5, 'I-LOC': 6, 'B-ORG': 7, 'B-PER': 8}
Training model
Model saved

Slovak
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'B-LOC': 0, 'I-PER': 1, 'I-ORG': 2, 'O': 3, 'I-LOC': 4, 'B-ORG': 5, 'B-PER': 6}
Training model
Model saved

Croatian
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'B-LOC': 0, 'I-PER': 1, 'I-ORG': 2, 'O': 3, 'B-OTH': 4, 'I-OTH': 5, 'I-LOC': 6, 'B-ORG': 7, 'B-PER': 8}
Training model
Model saved

English
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'B-LOC': 0, 'I-PER': 1, 'I-ORG': 2, 'O': 3, 'I-LOC': 4, 'B-ORG': 5, 'B-PER': 6}
Training model
Model saved

Danish
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'B-LOC': 0, 'I-PER': 1, 'I-ORG': 2, 'O': 3, 'I-LOC': 4, 'B-ORG': 5, 'B-PER': 6}
Training model
Model saved

Running time of the script: 10747.85 seconds

Process finished with exit code 0
